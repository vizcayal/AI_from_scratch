{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vector.shape = torch.Size([10, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\i34005\\AppData\\Local\\Temp\\ipykernel_1592\\3438091702.py:30: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  attn_weights = nn.functional.softmax(attn_scores)/torch.sqrt(n_head_tensor)     #(n_head, n_tokens, n_tokens)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0689,  0.0257,  0.1130,  ..., -0.0400, -0.0066, -0.0514],\n",
       "        [ 0.0220,  0.0993,  0.0570,  ..., -0.0787, -0.0307, -0.0619],\n",
       "        [ 0.0005,  0.0103,  0.1120,  ..., -0.0099, -0.0527, -0.0462],\n",
       "        ...,\n",
       "        [-0.0036,  0.0739,  0.0293,  ..., -0.0195, -0.0068, -0.0577],\n",
       "        [-0.0003,  0.0262,  0.1123,  ..., -0.0230,  0.0333, -0.0129],\n",
       "        [-0.0471, -0.0655,  0.1521,  ..., -0.0323, -0.1300, -0.0706]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "n_tokens = 10\n",
    "emb_dim = 256\n",
    "x = torch.randn(n_tokens, emb_dim)\n",
    "\n",
    "class MHA(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_heads = 4, emb_dim = emb_dim):\n",
    "        super(MHA, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        assert emb_dim % n_heads == 0, \"emb_dim must be divisible by n_heads\"\n",
    "        self.head_dim = emb_dim // n_heads\n",
    "        self.pq = torch.nn.Linear(self.head_dim, self.head_dim)\n",
    "        self.pk = torch.nn.Linear(self.head_dim, self.head_dim)\n",
    "        self.pv = torch.nn.Linear(self.head_dim, self.head_dim)\n",
    "        self.output = torch.nn.Linear(emb_dim, emb_dim)\n",
    "    \n",
    "    def forward(self, x):                                                               #(n_tokens, emb_dim)\n",
    "        x = x.reshape(n_tokens, self.n_heads, self.head_dim)                            #(n_tokens, n_head, head_dim)\n",
    "        x = x.permute(1, 0, 2)                                                          #(n_head, n_tokens, head_dim) \n",
    "        q = self.pq(x)                                                                  #(n_head, n_tokens, head_dim)\n",
    "        k = self.pk(x)                                                                  #(n_head, n_tokens, head_dim)\n",
    "        v = self.pv(x)                                                                  #(n_head, n_tokens, head_dim)\n",
    "        kt = torch.transpose(k, 1 ,2)                                                   #(n_head, head_dim, n_tokens)\n",
    "        attn_scores = q@kt                                                              #(n_head, n_tokens, n_tokens)\n",
    "        n_head_tensor = torch.tensor(self.head_dim, dtype= float)                       #(1)\n",
    "        attn_weights = nn.functional.softmax(attn_scores)/torch.sqrt(n_head_tensor)     #(n_head, n_tokens, n_tokens)\n",
    "        context_vector = attn_weights @ v                                               #(n_head, n_tokens, head_dim)\n",
    "        context_vector = context_vector.permute(1, 0, 2)                                #(n_tokens, n_head, head_dim)\n",
    "        context_vector = context_vector.contiguous()                                \n",
    "        context_vector = context_vector.view(n_tokens, self.head_dim * self.n_heads)    #(n_tokens, emb_dim)\n",
    "        context_vector = self.output(context_vector)\n",
    "        print(f'{context_vector.shape = }')\n",
    "        return(context_vector)\n",
    "        \n",
    "my_mha = MHA()\n",
    "my_mha(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
